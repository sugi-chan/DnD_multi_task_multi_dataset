{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from random import randrange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "dat_race = pd.read_csv('dnd_race.csv')\n",
    "dat_class = pd.read_csv('dnd_class.csv')\n",
    "\n",
    "dat_race['image_path'] = dat_race['label']+'/'+dat_race['image_number']\n",
    "dat_class['image_path'] = dat_class['label']+'/'+dat_class['image_number']\n",
    "\n",
    "dat_race = dat_race.rename(index=str, columns={\"label\": \"race\"})\n",
    "dat_class = dat_class.rename(index=str, columns={\"label\": \"class\"})\n",
    "dat_class.head(1)\n",
    "\n",
    "\n",
    "X_race = dat_race['image_path']\n",
    "X_class = dat_class['image_path']\n",
    "\n",
    "y_race = pd.get_dummies(dat_race['race'])\n",
    "y_class = pd.get_dummies(dat_class['class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rogue      148\n",
       "paladin    147\n",
       "wizard     119\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_class['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.357487922705314"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "148/dat_class.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "elf           188\n",
       "gnome         122\n",
       "dragonborn    120\n",
       "Name: race, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_race.race.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4372093023255814"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "188/dat_race.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_race_train, X_race_test, y_race_train, y_race_test = train_test_split(X_race, y_race, test_size=0.20, random_state=42)\n",
    "X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(X_class, y_class, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_race_train = X_race_train.values.tolist()\n",
    "X_race_test = X_race_test.values.tolist()\n",
    "\n",
    "\n",
    "X_class_train = X_class_train.values.tolist()\n",
    "X_class_test = X_class_test.values.tolist()\n",
    "\n",
    "race_shape = y_race_train.shape[1]\n",
    "class_shape = y_class_train.shape[1]\n",
    "\n",
    "\n",
    "y_race_train = y_race_train.values.tolist()\n",
    "y_race_test = y_race_test.values.tolist()\n",
    "\n",
    "y_class_train = y_class_train.values.tolist()\n",
    "y_class_test = y_class_test.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_race_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "331"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_class_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in dataset_style:\n",
    "#    print(i,dat[i].sum(),dat[i].sum()/dat.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in dataset_rest:\n",
    "#    print(i,dat[i].sum(),dat[i].sum()/dat.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "flip = random.randint(0, 1)\n",
    "flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fgo_dataset(Dataset):\n",
    "    def __init__(self,king_of_lists, transform=None):\n",
    "        \"\"\"\n",
    "        gender_img, style_img, gender_pred,style_pred \n",
    "        \n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.king_of_lists = king_of_lists\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        #random_index = randrange(len(self.king_of_lists[index]))\n",
    "        \n",
    "        img1 = Image.open('images/'+self.king_of_lists[0][index])\n",
    "        img1 = img1.convert('RGB')\n",
    "        \n",
    "        #img2 = Image.open('images/'+self.king_of_lists[1][index])\n",
    "        #img2 = img2.convert('RGB')\n",
    "        \n",
    "        \n",
    "        gender = self.king_of_lists[1][index] # gender\n",
    "        #color = self.king_of_lists[3][index] # region\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "\n",
    "        list_of_labels = [torch.from_numpy(np.array(gender))]\n",
    "       \n",
    "        return img1, list_of_labels[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.king_of_lists[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((256,256)),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        #transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "train_lists = [X_race_train, y_race_train]\n",
    "test_lists = [X_race_test, y_race_test ]\n",
    "\n",
    "training_dataset = fgo_dataset(king_of_lists = train_lists,\n",
    "                               transform = data_transforms['train'] )\n",
    "\n",
    "test_dataset = fgo_dataset(king_of_lists = test_lists,\n",
    "                           transform = data_transforms['val'] )\n",
    "\n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "                   'val':torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "                   }\n",
    "dataset_sizes = {'train':len(train_lists[0]),\n",
    "                'val':len(test_lists[0])}\n",
    "\n",
    "\n",
    "train_class_lists = [X_class_train, y_class_train]\n",
    "test_class_lists = [X_class_test, y_class_test ]\n",
    "\n",
    "training_style_dataset = fgo_dataset(king_of_lists = train_class_lists,\n",
    "                               transform = data_transforms['train'] )\n",
    "\n",
    "test_style_dataset = fgo_dataset(king_of_lists = test_class_lists,\n",
    "                           transform = data_transforms['val'] )\n",
    "\n",
    "dataloaders_style_dict = {'train': torch.utils.data.DataLoader(training_style_dataset, batch_size=batch_size,drop_last =True, shuffle=True, num_workers=0),\n",
    "                   'val':torch.utils.data.DataLoader(test_style_dataset, batch_size=batch_size,drop_last =True, shuffle=True, num_workers=0)\n",
    "                   }\n",
    "dataloaders_style_sizes = {'train':len(train_lists[0]),\n",
    "                'val':len(test_lists[0])}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 10\n"
     ]
    }
   ],
   "source": [
    "print(len(dataloaders_style_dict['train']),len(dataloaders_style_dict['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 11\n"
     ]
    }
   ],
   "source": [
    "print(len(dataloaders_dict['train']),len(dataloaders_dict['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders_dict[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    #_, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    loss = criterion(outputs, torch.max(labels.float(), 1)[1]).to(device)\n",
    "                    #loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects +=torch.sum(torch.max(outputs, 1)[1] == torch.max(labels, 1)[1])\n",
    "                #running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, './models/dnd_resnet_50.pth')\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 3)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.00001,weight_decay = .01)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\micha\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\PIL\\Image.py:965: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  ' expressed in bytes should be converted ' +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.1190 Acc: 0.3576\n",
      "val Loss: 1.0327 Acc: 0.5233\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.9740 Acc: 0.5843\n",
      "val Loss: 0.9094 Acc: 0.7326\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.8695 Acc: 0.7064\n",
      "val Loss: 0.8012 Acc: 0.7442\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.7860 Acc: 0.7238\n",
      "val Loss: 0.7141 Acc: 0.7326\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.6995 Acc: 0.7733\n",
      "val Loss: 0.6348 Acc: 0.7674\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.6342 Acc: 0.7936\n",
      "val Loss: 0.6146 Acc: 0.7791\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.5853 Acc: 0.7849\n",
      "val Loss: 0.6076 Acc: 0.7791\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.5611 Acc: 0.8081\n",
      "val Loss: 0.5981 Acc: 0.7791\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.6060 Acc: 0.7616\n",
      "val Loss: 0.6026 Acc: 0.7791\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.5599 Acc: 0.8285\n",
      "val Loss: 0.6081 Acc: 0.7674\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.5364 Acc: 0.8256\n",
      "val Loss: 0.5873 Acc: 0.7907\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.5496 Acc: 0.8459\n",
      "val Loss: 0.5929 Acc: 0.7791\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.5306 Acc: 0.8227\n",
      "val Loss: 0.5851 Acc: 0.7791\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.5603 Acc: 0.7936\n",
      "val Loss: 0.5796 Acc: 0.7791\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.5542 Acc: 0.8052\n",
      "val Loss: 0.5984 Acc: 0.7791\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.5296 Acc: 0.8285\n",
      "val Loss: 0.5864 Acc: 0.7791\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.5424 Acc: 0.8023\n",
      "val Loss: 0.5955 Acc: 0.7791\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.5251 Acc: 0.8198\n",
      "val Loss: 0.5974 Acc: 0.7791\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.5324 Acc: 0.8140\n",
      "val Loss: 0.5897 Acc: 0.8023\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.5614 Acc: 0.7907\n",
      "val Loss: 0.5783 Acc: 0.7907\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.5190 Acc: 0.8169\n",
      "val Loss: 0.5988 Acc: 0.7674\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.5145 Acc: 0.8343\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
